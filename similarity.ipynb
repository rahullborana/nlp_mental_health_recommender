{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rahul\\OneDrive\\Documents\\Fall\\Unstructured data analytics\\Final Project\\nlp_mental_health_recommender\\similarity.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mvader_lexicon\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m RegexpTokenizer, word_tokenize\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[39m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mErrorsWithCodes\u001b[39;00m(\u001b[39mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, code):\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\compat.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcPickle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mabout\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m registry\n\u001b[0;32m      7\u001b[0m \u001b[39m# fmt: off\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mregistry\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\config.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfection\u001b[39;00m \u001b[39mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m Decorator\n\u001b[0;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mregistry\u001b[39;00m(confection\u001b[39m.\u001b[39mregistry):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# fmt: off\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizers: Decorator \u001b[39m=\u001b[39m catalogue\u001b[39m.\u001b[39mcreate(\u001b[39m\"\u001b[39m\u001b[39mthinc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moptimizers\u001b[39m\u001b[39m\"\u001b[39m, entry_points\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\types.py:25\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Any,\n\u001b[0;32m      6\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     overload,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m cupy, has_cupy\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m has_cupy:\n\u001b[0;32m     28\u001b[0m     get_array_module \u001b[39m=\u001b[39m cupy\u001b[39m.\u001b[39mget_array_module\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\compat.py:54\u001b[0m\n\u001b[0;32m     51\u001b[0m     torch_version \u001b[39m=\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m0.0.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdlpack\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     has_tensorflow \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m cancellation\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m execute\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m executor\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_conversion_registry\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32mc:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m \u001b[39mimport\u001b[39;00m trace_type\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocs\u001b[39;00m \u001b[39mimport\u001b[39;00m doc_controls\n\u001b[1;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtsl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_bfloat16\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m cpp_shape_inference_pb2\n\u001b[0;32m     38\u001b[0m _np_bfloat16 \u001b[39m=\u001b[39m pywrap_bfloat16\u001b[39m.\u001b[39mbfloat16_type()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disorder_name = input(\"Choose one of anxiety, bipolar-disorder ,clinical-depression, post-traumatic-stress-disorder and obsessive-compulsive-disorder\")\n",
    "\n",
    "disorder_name='anxiety'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_rant=input(\"Tell us how you feel!\")\n",
    "\n",
    "user_rant= 'i feel anxious'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rahul\\OneDrive\\Documents\\Fall\\Unstructured data analytics\\Final Project\\nlp_mental_health_recommender\\similarity.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_excel(\u001b[39m'\u001b[39m\u001b[39mmental_health_data.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.read_excel('mental_health_data.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        hello.  i am a 51 y/o man, suffering from bipo...\n",
      "1        hello.  i am a 51 y/o man, suffering from bipo...\n",
      "2        trigger warning‚ä¶,having a breakdown of crisi...\n",
      "3        trigger warning‚ä¶,having a breakdown of crisi...\n",
      "4        trigger warning‚ä¶,having a breakdown of crisi...\n",
      "                               ...                        \n",
      "29049    i haven't been active on here in awhile...ice ...\n",
      "29050    i haven't been active on here in awhile...ice ...\n",
      "29051    i haven't been active on here in awhile...ice ...\n",
      "29052    i haven't been active on here in awhile...ice ...\n",
      "29053    i haven't been active on here in awhile...ice ...\n",
      "Name: Concern, Length: 29054, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_words = [word for word in filtered_words if word.isalnum()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Assuming df['Concern'] contains your text data\n",
    "df['Concern'] = df['Concern'].str.lower()\n",
    "# df['Concern'] = df['Concern'].astype(str).apply(remove_stopwords)\n",
    "print(df['Concern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disorder=df[df[\"tag\"]==disorder_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disorder= df_disorder[['Concern']].drop_duplicates().reset_index(drop=True)\n",
    "df_disorder['User Concern']=user_rant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concern</th>\n",
       "      <th>User Concern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beginning to use some not good coping skills. ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi everyone. i just joined this forum due to w...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the description below  is me.\"exercise and str...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does anyone feel like you concentrate on one t...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just called my grandparents to tell them im ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>has anyone here had prolia ( denosumab ) as th...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>\"anxiety, being what anxiety is, worked its wa...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>i've seen a similar post on here but it's not ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>so i've experienced general health anxiety for...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>i hope this is ok to post, i know there are al...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1399 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Concern    User Concern\n",
       "0     beginning to use some not good coping skills. ...  i feel anxious\n",
       "1     hi everyone. i just joined this forum due to w...  i feel anxious\n",
       "2     the description below  is me.\"exercise and str...  i feel anxious\n",
       "3     does anyone feel like you concentrate on one t...  i feel anxious\n",
       "4     i just called my grandparents to tell them im ...  i feel anxious\n",
       "...                                                 ...             ...\n",
       "1394  has anyone here had prolia ( denosumab ) as th...  i feel anxious\n",
       "1395  \"anxiety, being what anxiety is, worked its wa...  i feel anxious\n",
       "1396  i've seen a similar post on here but it's not ...  i feel anxious\n",
       "1397  so i've experienced general health anxiety for...  i feel anxious\n",
       "1398  i hope this is ok to post, i know there are al...  i feel anxious\n",
       "\n",
       "[1399 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concern</th>\n",
       "      <th>User Concern</th>\n",
       "      <th>Concern_Sentiment</th>\n",
       "      <th>User_Concern_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beginning to use some not good coping skills. ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.6043</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi everyone. i just joined this forum due to w...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.3762</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the description below  is me.\"exercise and str...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.5212</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does anyone feel like you concentrate on one t...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.6980</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just called my grandparents to tell them im ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.9590</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>has anyone here had prolia ( denosumab ) as th...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>\"anxiety, being what anxiety is, worked its wa...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.9590</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>i've seen a similar post on here but it's not ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>so i've experienced general health anxiety for...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.9874</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>i hope this is ok to post, i know there are al...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1399 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Concern    User Concern  \\\n",
       "0     beginning to use some not good coping skills. ...  i feel anxious   \n",
       "1     hi everyone. i just joined this forum due to w...  i feel anxious   \n",
       "2     the description below  is me.\"exercise and str...  i feel anxious   \n",
       "3     does anyone feel like you concentrate on one t...  i feel anxious   \n",
       "4     i just called my grandparents to tell them im ...  i feel anxious   \n",
       "...                                                 ...             ...   \n",
       "1394  has anyone here had prolia ( denosumab ) as th...  i feel anxious   \n",
       "1395  \"anxiety, being what anxiety is, worked its wa...  i feel anxious   \n",
       "1396  i've seen a similar post on here but it's not ...  i feel anxious   \n",
       "1397  so i've experienced general health anxiety for...  i feel anxious   \n",
       "1398  i hope this is ok to post, i know there are al...  i feel anxious   \n",
       "\n",
       "      Concern_Sentiment  User_Concern_Sentiment  \n",
       "0                0.6043                   -0.25  \n",
       "1               -0.3762                   -0.25  \n",
       "2               -0.5212                   -0.25  \n",
       "3               -0.6980                   -0.25  \n",
       "4               -0.9590                   -0.25  \n",
       "...                 ...                     ...  \n",
       "1394             0.6335                   -0.25  \n",
       "1395            -0.9590                   -0.25  \n",
       "1396             0.9425                   -0.25  \n",
       "1397            -0.9874                   -0.25  \n",
       "1398             0.8735                   -0.25  \n",
       "\n",
       "[1399 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate the sentiment score for the single \"User Concern\" value\n",
    "user_concern_sentiment = analyzer.polarity_scores(df_disorder.iloc[0]['User Concern'])['compound']\n",
    "\n",
    "# Apply the sentiment score to all rows in the \"Concern\" column\n",
    "\n",
    "df_disorder['Concern_Sentiment'] = df_disorder['Concern'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "df_disorder['User_Concern_Sentiment'] = user_concern_sentiment\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that a sentiment score of greater than 0.05 (based on Vader) of the user rant means the user is talking about something cheerful\n",
    "# Same with all concerns in our data\n",
    "\n",
    "# In other words, we need to filter our for negative user rants and negative concerns in our data\n",
    "\n",
    "df_disorder= df_disorder[(df_disorder['Concern_Sentiment']<0.05) & (df_disorder['User_Concern_Sentiment']<0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disorder=df_disorder[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_40176\\3849683405.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_disorder['Similarity Score'] = df_disorder['Concern'].apply(word_vector_similarity, text2=user_rant)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "def word_vector_similarity(text1, text2):\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Create a new DataFrame with product name, product review, and similarity score\n",
    "# similarity_score = pd.DataFrame({\n",
    "#     'Product Name': df['item'],\n",
    "#     'Concern': df_disorder['Concern'],\n",
    "#     'Similarity Score': ''\n",
    "# })\n",
    "\n",
    "# this adds the simlarity score of each comment in the concern column with the user input\n",
    "df_disorder['Similarity Score'] = df_disorder['Concern'].apply(word_vector_similarity, text2=user_rant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disorder=df_disorder[df_disorder['Similarity Score']>0.5].sort_values(by='Similarity Score', ascending=False).reset_index(drop=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "# from gensim.models.phrases import Phraser\n",
    "# from gensim.models import LdaModel\n",
    "# from gensim import corpora\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].apply(remove_stopwords)\n",
    "\n",
    "# # Create bigrams or multi-word phrases\n",
    "# bigram = Phrases(df_sug['Suggestion'], min_count=5, threshold=100)\n",
    "# bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# # Preprocess the text and apply the bigram phraser\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].str.lower()\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(word_tokenize)\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(lambda tokens: bigram_phraser[tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sug['Suggestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary and a corpus for LDA\n",
    "# dictionary = corpora.Dictionary(df_sug['Suggestion'])\n",
    "# corpus = [dictionary.doc2bow(text) for text in df_sug['Suggestion']]\n",
    "\n",
    "# # Train an LDA model\n",
    "# num_topics = 2  # Set the number of topics you want\n",
    "# lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# # Print the top phrases for each topic\n",
    "# for topic_num in range(num_topics):\n",
    "#     topic_words = lda_model.show_topic(topic_num)\n",
    "#     print(f\"Topic {topic_num + 1}:\")\n",
    "#     for word, score in topic_words:\n",
    "#         print(f\"{word}: {score}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "# from gensim.models.phrases import Phraser\n",
    "# from gensim.models import LdaModel\n",
    "# from gensim import corpora\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].apply(remove_stopwords)\n",
    "\n",
    "# # Tokenize the sentences in the 'Suggestion' column\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].str.lower()\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(word_tokenize)\n",
    "\n",
    "# # Create bigrams or multi-word phrases\n",
    "# bigram = Phrases(df_sug['Suggestion'], min_count=2, threshold=20)\n",
    "# bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# # Apply the bigram phraser to the tokenized text\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(lambda tokens: bigram_phraser[tokens])\n",
    "\n",
    "# # Create a dictionary and a corpus for LDA\n",
    "# dictionary = corpora.Dictionary(df_sug['Suggestion'])\n",
    "# corpus = [dictionary.doc2bow(text) for text in df_sug['Suggestion']]\n",
    "\n",
    "# # Train an LDA model\n",
    "# num_topics = 2  # Set the number of topics you want\n",
    "# lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# # Print the top phrases for each topic\n",
    "# for topic_num in range(num_topics):\n",
    "#     topic_words = lda_model.show_topic(topic_num)\n",
    "#     print(f\"Topic {topic_num + 1}:\")\n",
    "#     for word, score in topic_words:\n",
    "#         print(f\"{word}: {score}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "# # df_sug['Suggestion']= df_sug['Suggestion'].apply(remove_stopwords)\n",
    "# # Define a function to extract noun phrases\n",
    "# def extract_noun_phrases(text):\n",
    "#     words = word_tokenize(text)\n",
    "#     tagged = pos_tag(words)\n",
    "#     grammar = r\"\"\"\n",
    "#         NP: {<DT>?<JJ>*<NN.*>+}\n",
    "#     \"\"\"\n",
    "#     cp = RegexpParser(grammar)\n",
    "#     tree = cp.parse(tagged)\n",
    "#     noun_phrases = []\n",
    "#     for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "#         noun_phrase = \" \".join(word for word, tag in subtree.leaves())\n",
    "#         noun_phrases.append(noun_phrase)\n",
    "#     return noun_phrases\n",
    "\n",
    "# # Apply the function to each suggestion\n",
    "# df_sug['Noun_Phrases'] = df_sug['Suggestion'].apply(extract_noun_phrases)\n",
    "\n",
    "# # Print the extracted noun phrases\n",
    "# print(df_sug['Noun_Phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# # Load a pre-trained spaCy model (English)\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# # Extract unique verbs from all suggestions\n",
    "# unique_verbs = set()  # Initialize an empty set to store unique verbs\n",
    "\n",
    "# for suggestion in df_sug['Suggestion']:\n",
    "#     # Process the text with spaCy\n",
    "#     doc = nlp(suggestion)\n",
    "    \n",
    "#     # Extract verbs (tokens with the POS tag \"VERB\") and add them to the set\n",
    "#     extracted_verbs = {token.text for token in doc if token.pos_ == \"VERB\"}\n",
    "    \n",
    "#     # Update the set of unique verbs with the extracted verbs\n",
    "#     unique_verbs.update(extracted_verbs)\n",
    "\n",
    "# # Convert the set of unique verbs back to a list (if needed)\n",
    "# unique_verbs_list = list(unique_verbs)\n",
    "\n",
    "# # Print the unique set of verbs\n",
    "# print(unique_verbs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "\n",
    "# # Load a pre-trained spaCy model (English)\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# # Define a function to filter verbs\n",
    "# def filter_verbs(text):\n",
    "#     doc = nlp(text)\n",
    "#     verb_tokens = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "#     return \" \".join(verb_tokens)\n",
    "\n",
    "# # Apply the filter to the DataFrame\n",
    "# df_sug['Filtered_Text'] = df_sug['Suggestion'].apply(filter_verbs)\n",
    "\n",
    "# # Create a TF-IDF vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=10)  # Adjust the number of features/topics as needed\n",
    "\n",
    "# # Fit and transform the filtered text data\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(df_sug['Filtered_Text'])\n",
    "\n",
    "# # Apply NMF for topic modeling\n",
    "# num_topics = 2  # Set the number of topics you want\n",
    "# nmf_model = NMF(n_components=num_topics)\n",
    "# nmf_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# # Get the top words for each topic\n",
    "# feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# top_words_per_topic = []\n",
    "# for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "#     top_words_idx = topic.argsort()[-5:][::-1]  # Adjust the number of top words per topic as needed\n",
    "#     top_words = [feature_names[i] for i in top_words_idx]\n",
    "#     top_words_per_topic.append(top_words)\n",
    "\n",
    "# # Print the top words for each topic\n",
    "# for topic_idx, top_words in enumerate(top_words_per_topic):\n",
    "#     print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# # Combine rows into one paragraph\n",
    "# paragraph = ' '.join(df_sug['Suggestion'])\n",
    "\n",
    "# # Load a pre-trained spaCy model (English)\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "# nlp.max_length = 2000000 # or even higher\n",
    "\n",
    "# # Split the paragraph into sentences\n",
    "# doc = nlp(paragraph)\n",
    "# sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# # Initialize a matrix to store sentence similarity scores\n",
    "# num_sentences = len(sentences)\n",
    "# similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "# # Calculate sentence similarity using word vector similarity\n",
    "# for i in range(num_sentences):\n",
    "#     for j in range(i, num_sentences):  # Only calculate each pair once\n",
    "#         if i != j:\n",
    "#             similarity = nlp(sentences[i]).similarity(nlp(sentences[j]))\n",
    "#             similarity_matrix[i, j] = similarity\n",
    "#             similarity_matrix[j, i] = similarity  # Symmetric\n",
    "\n",
    "# # Print the sentence similarity matrix (optional)\n",
    "# print(\"Sentence Similarity Matrix:\")\n",
    "# print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# Combine rows into one paragraph\n",
    "paragraph = ' '.join(df_sug['Suggestion'])\n",
    "\n",
    "# Load a pre-trained spaCy model (English)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 2000000 # or even higher\n",
    "\n",
    "# Split the paragraph into sentences\n",
    "doc = nlp(paragraph)\n",
    "sentences = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Similarity Matrix:\n",
      "[[0.         0.67193466 0.73401952 ... 0.5498715  0.73435664 0.6273554 ]\n",
      " [0.67193466 0.         0.80114758 ... 0.81205541 0.70404387 0.79551411]\n",
      " [0.73401952 0.80114758 0.         ... 0.84287786 0.82239437 0.83103955]\n",
      " ...\n",
      " [0.5498715  0.81205541 0.84287786 ... 0.         0.76752681 0.89502686]\n",
      " [0.73435664 0.70404387 0.82239437 ... 0.76752681 0.         0.79425079]\n",
      " [0.6273554  0.79551411 0.83103955 ... 0.89502686 0.79425079 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_sug = df_disorder.merge(df, on='Concern', how='left')[['Concern', 'User Concern', 'Similarity Score', 'Concern_Sentiment', 'User_Concern_Sentiment', 'Suggestion']]\n",
    "df_sug['Suggestion'] = df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# Combine rows into one paragraph\n",
    "paragraph = ' '.join(df_sug['Suggestion'])\n",
    "\n",
    "# Load a pre-trained spaCy model (English)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 2000000  # or even higher\n",
    "\n",
    "# Split the paragraph into sentences\n",
    "doc = nlp(paragraph)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Initialize a matrix to store sentence similarity scores\n",
    "num_sentences = len(sentences)\n",
    "similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "# Calculate sentence similarity using cosine similarity\n",
    "for i in range(num_sentences):\n",
    "    for j in range(i, num_sentences):  # Only calculate each pair once\n",
    "        if i != j:\n",
    "            vector1 = nlp(sentences[i]).vector\n",
    "            vector2 = nlp(sentences[j]).vector\n",
    "            similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity  # Symmetric\n",
    "\n",
    "# Print the sentence similarity matrix (optional)\n",
    "print(\"Sentence Similarity Matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: You need to understand yourself  and heal  and accept any past problems, thiscan of course  be a  problem and you need that support that seems to be lacking  at this time BOB Hi, can I just say I feel for you, my husband also suffers severe depression & I know  from experience what your saying.\n",
      "Top 2: so I am limited Also' for me' People You have nothing to be sorry for, my dear, you must not take the blame for that which you did not do, though those who are to blame will readily let you burden yourself with their guilt.\n",
      "Top 3: There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.\n",
      "Top 4: You may try, I do this and it helps me a bit, to take a list of what is done to you and what you do to others.\n",
      "Top 5: No one will ever understand what going through mental health issues are like, so why bother to dig the hole deeper for yourself.  \n",
      "Top 6: hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat\n",
      "Top 7: I think mostly you need to keep talking, no matter how silly it sounds in your head.\n",
      "Top 8: Never in the right place, always feeling like you are demanding to people and feeling like you annoy them.\n",
      "Top 9: Stepping back to take stock of all you are dealing with, in a way that acknowledges your incredible strength is important because it shows you that you are not as fragile as you may feel.\n",
      "Top 10: We don't always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it's worth trying to find out what it is that people don't stay for, and are they really the people you would want to stay for long.   \n"
     ]
    }
   ],
   "source": [
    "# Sum up the similarity values for each sentence\n",
    "sentence_similarity_scores = np.sum(similarity_matrix, axis=1)\n",
    "\n",
    "# Create a list of (sentence, score) tuples\n",
    "sentence_scores_tuples = [(sentence, score) for sentence, score in zip(sentences, sentence_similarity_scores)]\n",
    "\n",
    "# Sort the tuples by score in descending order\n",
    "sentence_scores_tuples_sorted = sorted(sentence_scores_tuples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 10 sentences with the highest scores\n",
    "top_10_sentences = [sentence for sentence, score in sentence_scores_tuples_sorted[:10]]\n",
    "\n",
    "# Print the top 10 sentences\n",
    "for i, sentence in enumerate(top_10_sentences, 1):\n",
    "    print(f\"Top {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(top_10_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need to understand yourself  and heal  and accept any past problems, thiscan of course  be a  problem and you need that support that seems to be lacking  at this time BOB Hi, can I just say I feel for you, my husband also suffers severe depression & I know  from experience what your saying. hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat We don't always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it's worth trying to find out what it is that people don't stay for, and are they really the people you would want to stay for long.   \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Your long text goes here\n",
    "text = ' '.join(top_10_sentences)\n",
    "\n",
    "# Tokenize the text and remove stop words\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc if token.text not in STOP_WORDS]\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "# Calculate sentence scores based on word frequency\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "sentence_scores = {sent: sum([word_freq[word] for word in sent.split()]) for sent in sentences}\n",
    "\n",
    "# Get the top N sentences for summary\n",
    "summary_sentences = [sentence for sentence, score in sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:3]]\n",
    "\n",
    "# Join the summary sentences to create the summary\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You need to understand yourself  and heal  and accept any past problems, thiscan of course  be a  problem and you need that support that seems to be lacking  at this time BOB Hi, can I just say I feel for you, my husband also suffers severe depression & I know  from experience what your saying.',\n",
       " \"so I am limited Also' for me' People You have nothing to be sorry for, my dear, you must not take the blame for that which you did not do, though those who are to blame will readily let you burden yourself with their guilt.\",\n",
       " 'There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.',\n",
       " 'You may try, I do this and it helps me a bit, to take a list of what is done to you and what you do to others.',\n",
       " 'No one will ever understand what going through mental health issues are like, so why bother to dig the hole deeper for yourself.  ',\n",
       " 'hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat',\n",
       " 'I think mostly you need to keep talking, no matter how silly it sounds in your head.',\n",
       " 'Never in the right place, always feeling like you are demanding to people and feeling like you annoy them.',\n",
       " 'Stepping back to take stock of all you are dealing with, in a way that acknowledges your incredible strength is important because it shows you that you are not as fragile as you may feel.',\n",
       " 'We don\\'t always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it\\'s worth trying to find out what it is that people don\\'t stay for, and are they really the people you would want to stay for long.   ']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Phrase Detected: You need to understand yourself  and heal  and accept any past problems, thiscan of course  be a  problem and you need that support that seems to be lacking  at this time BOB Hi, can I just say I feel for you, my husband also suffers severe depression & I know  from experience what your saying.\n",
      "Action Phrase Detected: so I am limited Also' for me' People You have nothing to be sorry for, my dear, you must not take the blame for that which you did not do, though those who are to blame will readily let you burden yourself with their guilt.\n",
      "Action Phrase Detected: There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.\n",
      "Action Phrase Detected: You may try, I do this and it helps me a bit, to take a list of what is done to you and what you do to others.\n",
      "Action Phrase Detected: No one will ever understand what going through mental health issues are like, so why bother to dig the hole deeper for yourself.\n",
      "Action Phrase Detected: hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat I think mostly you need to keep talking, no matter how silly it sounds in your head.\n",
      "Action Phrase Detected: Never in the right place, always feeling like you are demanding to people and feeling like you annoy them.\n",
      "Action Phrase Detected: Stepping back to take stock of all you are dealing with, in a way that acknowledges your incredible strength is important because it shows you that you are not as fragile as you may feel.\n",
      "Action Phrase Detected: We don't always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it's worth trying to find out what it is that people don't stay for, and are they really the people you would want to stay for long.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph= ' '.join(top_10_sentences)\n",
    "# Split paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Define a threshold for action phrase detection\n",
    "threshold = 0.5\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    # Tokenize and convert to tensor\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True, max_length=64, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        action_probability = torch.softmax(outputs.logits, dim=1)[0][1].item()\n",
    "\n",
    "    # Check if the sentence contains an action phrase\n",
    "    is_action_phrase = action_probability > threshold\n",
    "\n",
    "    if is_action_phrase:\n",
    "        print(\"Action Phrase Detected:\", sentence)\n",
    "    else:\n",
    "        print(\"Not an Action Phrase:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rahul\\OneDrive\\Documents\\Fall\\Unstructured data analytics\\Final Project\\nlp_mental_health_recommender\\similarity.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rahul/OneDrive/Documents/Fall/Unstructured%20data%20analytics/Final%20Project/nlp_mental_health_recommender/similarity.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m2\u001b[39m:\u001b[39m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Phrases: ['need to understand', 'heal', 'accept', 'be', 'need', 'seems to be lacking', 'can', 'just say', 'feel', 'also suffers', 'know', 'so', 'am', 'Also', 'have', 'to be', 'must not take', 'did not do', 'are to blame will readily let', 'burden', 'is', 'as', 'forgiving', 'keeps doing', 'over', 'know', 'is', 'so', 'forgave', 'distanced', 'gave', 'to heal', 'were', 'may try', 'do', 'helps', 'to take', 'is done', 'do', 'will ever understand', 'going', 'are', 'so', 'bother to dig', 'deeper', 'there', 'think', 'fearing', 'going', 'right now', 'are', 'get', 'tell', 'it‚Äôs effecting', 'always fear', 'can‚Äôt', 'will pass', 'can always ring', 'pretty', 'talking', 'Take', 'think mostly', 'need to keep talking', 'no matter', 'sounds', 'Never', 'always feeling', 'are demanding', 'feeling', 'annoy', 'Stepping back to take', 'are dealing', 'acknowledges', 'is', 'shows', 'are not as', 'may feel', \"do n't always know\", 'seem', 'once said', 'was', 'actually', 'was', \"'s\", 'trying to find', 'is', \"do n't stay\", 'are', 'really', 'would want to stay', 'long']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Sample text\n",
    "text = ' '.join(top_10_sentences)\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define a list of action-related entity labels\n",
    "action_labels = [\"VERB\", \"AUX\", \"ADV\", \"PART\"]\n",
    "\n",
    "# Initialize a list to store action phrases\n",
    "action_phrases = []\n",
    "\n",
    "# Initialize variables to track phrases\n",
    "current_phrase = \"\"\n",
    "phrase_tokens = []\n",
    "\n",
    "# Iterate through tokens and extract action-related phrases\n",
    "for token in doc:\n",
    "    if token.pos_ in action_labels:\n",
    "        # If the token is an action-related word, add it to the current phrase\n",
    "        phrase_tokens.append(token.text)\n",
    "    else:\n",
    "        # If the token is not an action-related word, check if the current phrase is non-empty\n",
    "        if phrase_tokens:\n",
    "            # Join the tokens to form a phrase and add it to the list of action phrases\n",
    "            action_phrases.append(\" \".join(phrase_tokens))\n",
    "            # Reset variables for the next phrase\n",
    "            phrase_tokens = []\n",
    "\n",
    "# Check if there's a remaining phrase at the end\n",
    "if phrase_tokens:\n",
    "    action_phrases.append(\" \".join(phrase_tokens))\n",
    "\n",
    "# Print extracted action phrases\n",
    "print(\"Action Phrases:\", action_phrases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
