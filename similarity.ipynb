{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rachana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rachana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rachana/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disorder_name = input(\"Choose one of anxiety, bipolar-disorder ,clinical-depression, post-traumatic-stress-disorder and obsessive-compulsive-disorder\")\n",
    "\n",
    "disorder_name='anxiety'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_rant=input(\"Tell us how you feel!\")\n",
    "\n",
    "user_rant= 'i feel anxious'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>Concern</th>\n",
       "      <th>Suggestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Medication experiences</td>\n",
       "      <td>obsessive-compulsive-disorder-ocd</td>\n",
       "      <td>Hello.  I am a 51 y/o man, suffering from Bipo...</td>\n",
       "      <td>drugs.com says their are no interaction issues.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medication experiences</td>\n",
       "      <td>obsessive-compulsive-disorder-ocd</td>\n",
       "      <td>Hello.  I am a 51 y/o man, suffering from Bipo...</td>\n",
       "      <td>I've been on Cymbalta against my will for 9 ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Depression/OCD crisis</td>\n",
       "      <td>obsessive-compulsive-disorder-ocd</td>\n",
       "      <td>Trigger warning‚Ä¶,Having a breakdown of crisi...</td>\n",
       "      <td>I feel that. I was for real having a breakdown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Depression/OCD crisis</td>\n",
       "      <td>obsessive-compulsive-disorder-ocd</td>\n",
       "      <td>Trigger warning‚Ä¶,Having a breakdown of crisi...</td>\n",
       "      <td>I also have OCD so I get it, I really do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depression/OCD crisis</td>\n",
       "      <td>obsessive-compulsive-disorder-ocd</td>\n",
       "      <td>Trigger warning‚Ä¶,Having a breakdown of crisi...</td>\n",
       "      <td>üò•üò•‚ù§üíî</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                                tag  \\\n",
       "0  Medication experiences  obsessive-compulsive-disorder-ocd   \n",
       "1  Medication experiences  obsessive-compulsive-disorder-ocd   \n",
       "2   Depression/OCD crisis  obsessive-compulsive-disorder-ocd   \n",
       "3   Depression/OCD crisis  obsessive-compulsive-disorder-ocd   \n",
       "4   Depression/OCD crisis  obsessive-compulsive-disorder-ocd   \n",
       "\n",
       "                                             Concern  \\\n",
       "0  Hello.  I am a 51 y/o man, suffering from Bipo...   \n",
       "1  Hello.  I am a 51 y/o man, suffering from Bipo...   \n",
       "2  Trigger warning‚Ä¶,Having a breakdown of crisi...   \n",
       "3  Trigger warning‚Ä¶,Having a breakdown of crisi...   \n",
       "4  Trigger warning‚Ä¶,Having a breakdown of crisi...   \n",
       "\n",
       "                                          Suggestion  \n",
       "0    drugs.com says their are no interaction issues.  \n",
       "1  I've been on Cymbalta against my will for 9 ye...  \n",
       "2  I feel that. I was for real having a breakdown...  \n",
       "3           I also have OCD so I get it, I really do  \n",
       "4                                    üò•üò•‚ù§üíî  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('mental_health_data.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        hello.  i am a 51 y/o man, suffering from bipo...\n",
      "1        hello.  i am a 51 y/o man, suffering from bipo...\n",
      "2        trigger warning‚ä¶,having a breakdown of crisi...\n",
      "3        trigger warning‚ä¶,having a breakdown of crisi...\n",
      "4        trigger warning‚ä¶,having a breakdown of crisi...\n",
      "                               ...                        \n",
      "29049    i haven't been active on here in awhile...ice ...\n",
      "29050    i haven't been active on here in awhile...ice ...\n",
      "29051    i haven't been active on here in awhile...ice ...\n",
      "29052    i haven't been active on here in awhile...ice ...\n",
      "29053    i haven't been active on here in awhile...ice ...\n",
      "Name: Concern, Length: 29054, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_words = [word for word in filtered_words if word.isalnum()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Assuming df['Concern'] contains your text data\n",
    "df['Concern'] = df['Concern'].str.lower()\n",
    "# df['Concern'] = df['Concern'].astype(str).apply(remove_stopwords)\n",
    "print(df['Concern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disorder=df[df[\"tag\"]==disorder_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disorder= df_disorder[['Concern']].drop_duplicates().reset_index(drop=True)\n",
    "df_disorder['User Concern']=user_rant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concern</th>\n",
       "      <th>User Concern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beginning to use some not good coping skills. ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi everyone. i just joined this forum due to w...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the description below  is me.\"exercise and str...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does anyone feel like you concentrate on one t...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just called my grandparents to tell them im ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>has anyone here had prolia ( denosumab ) as th...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>\"anxiety, being what anxiety is, worked its wa...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>i've seen a similar post on here but it's not ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>so i've experienced general health anxiety for...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>i hope this is ok to post, i know there are al...</td>\n",
       "      <td>i feel anxious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1399 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Concern    User Concern\n",
       "0     beginning to use some not good coping skills. ...  i feel anxious\n",
       "1     hi everyone. i just joined this forum due to w...  i feel anxious\n",
       "2     the description below  is me.\"exercise and str...  i feel anxious\n",
       "3     does anyone feel like you concentrate on one t...  i feel anxious\n",
       "4     i just called my grandparents to tell them im ...  i feel anxious\n",
       "...                                                 ...             ...\n",
       "1394  has anyone here had prolia ( denosumab ) as th...  i feel anxious\n",
       "1395  \"anxiety, being what anxiety is, worked its wa...  i feel anxious\n",
       "1396  i've seen a similar post on here but it's not ...  i feel anxious\n",
       "1397  so i've experienced general health anxiety for...  i feel anxious\n",
       "1398  i hope this is ok to post, i know there are al...  i feel anxious\n",
       "\n",
       "[1399 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concern</th>\n",
       "      <th>User Concern</th>\n",
       "      <th>Concern_Sentiment</th>\n",
       "      <th>User_Concern_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beginning to use some not good coping skills. ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.6043</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi everyone. i just joined this forum due to w...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.3762</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the description below  is me.\"exercise and str...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.5212</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does anyone feel like you concentrate on one t...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.6980</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just called my grandparents to tell them im ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.9590</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>has anyone here had prolia ( denosumab ) as th...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>\"anxiety, being what anxiety is, worked its wa...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.9590</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>i've seen a similar post on here but it's not ...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>so i've experienced general health anxiety for...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>-0.9874</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>i hope this is ok to post, i know there are al...</td>\n",
       "      <td>i feel anxious</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1399 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Concern    User Concern  \\\n",
       "0     beginning to use some not good coping skills. ...  i feel anxious   \n",
       "1     hi everyone. i just joined this forum due to w...  i feel anxious   \n",
       "2     the description below  is me.\"exercise and str...  i feel anxious   \n",
       "3     does anyone feel like you concentrate on one t...  i feel anxious   \n",
       "4     i just called my grandparents to tell them im ...  i feel anxious   \n",
       "...                                                 ...             ...   \n",
       "1394  has anyone here had prolia ( denosumab ) as th...  i feel anxious   \n",
       "1395  \"anxiety, being what anxiety is, worked its wa...  i feel anxious   \n",
       "1396  i've seen a similar post on here but it's not ...  i feel anxious   \n",
       "1397  so i've experienced general health anxiety for...  i feel anxious   \n",
       "1398  i hope this is ok to post, i know there are al...  i feel anxious   \n",
       "\n",
       "      Concern_Sentiment  User_Concern_Sentiment  \n",
       "0                0.6043                   -0.25  \n",
       "1               -0.3762                   -0.25  \n",
       "2               -0.5212                   -0.25  \n",
       "3               -0.6980                   -0.25  \n",
       "4               -0.9590                   -0.25  \n",
       "...                 ...                     ...  \n",
       "1394             0.6335                   -0.25  \n",
       "1395            -0.9590                   -0.25  \n",
       "1396             0.9425                   -0.25  \n",
       "1397            -0.9874                   -0.25  \n",
       "1398             0.8735                   -0.25  \n",
       "\n",
       "[1399 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate the sentiment score for the single \"User Concern\" value\n",
    "user_concern_sentiment = analyzer.polarity_scores(df_disorder.iloc[0]['User Concern'])['compound']\n",
    "\n",
    "# Apply the sentiment score to all rows in the \"Concern\" column\n",
    "\n",
    "df_disorder['Concern_Sentiment'] = df_disorder['Concern'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "df_disorder['User_Concern_Sentiment'] = user_concern_sentiment\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_disorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that a sentiment score of greater than 0.05 (based on Vader) of the user rant means the user is talking about something cheerful\n",
    "# Same with all concerns in our data\n",
    "\n",
    "# In other words, we need to filter our for negative user rants and negative concerns in our data\n",
    "\n",
    "df_disorder= df_disorder[(df_disorder['Concern_Sentiment']<0.05) & (df_disorder['User_Concern_Sentiment']<0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disorder=df_disorder[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/1qwqb5150yx0cy6skpt709w80000gn/T/ipykernel_68434/3849683405.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_disorder['Similarity Score'] = df_disorder['Concern'].apply(word_vector_similarity, text2=user_rant)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "def word_vector_similarity(text1, text2):\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Create a new DataFrame with product name, product review, and similarity score\n",
    "# similarity_score = pd.DataFrame({\n",
    "#     'Product Name': df['item'],\n",
    "#     'Concern': df_disorder['Concern'],\n",
    "#     'Similarity Score': ''\n",
    "# })\n",
    "\n",
    "# this adds the simlarity score of each comment in the concern column with the user input\n",
    "df_disorder['Similarity Score'] = df_disorder['Concern'].apply(word_vector_similarity, text2=user_rant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disorder=df_disorder[df_disorder['Similarity Score']>0.5].sort_values(by='Similarity Score', ascending=False).reset_index(drop=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "# from gensim.models.phrases import Phraser\n",
    "# from gensim.models import LdaModel\n",
    "# from gensim import corpora\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].apply(remove_stopwords)\n",
    "\n",
    "# # Create bigrams or multi-word phrases\n",
    "# bigram = Phrases(df_sug['Suggestion'], min_count=5, threshold=100)\n",
    "# bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# # Preprocess the text and apply the bigram phraser\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].str.lower()\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(word_tokenize)\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(lambda tokens: bigram_phraser[tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sug['Suggestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary and a corpus for LDA\n",
    "# dictionary = corpora.Dictionary(df_sug['Suggestion'])\n",
    "# corpus = [dictionary.doc2bow(text) for text in df_sug['Suggestion']]\n",
    "\n",
    "# # Train an LDA model\n",
    "# num_topics = 2  # Set the number of topics you want\n",
    "# lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# # Print the top phrases for each topic\n",
    "# for topic_num in range(num_topics):\n",
    "#     topic_words = lda_model.show_topic(topic_num)\n",
    "#     print(f\"Topic {topic_num + 1}:\")\n",
    "#     for word, score in topic_words:\n",
    "#         print(f\"{word}: {score}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "# from gensim.models.phrases import Phraser\n",
    "# from gensim.models import LdaModel\n",
    "# from gensim import corpora\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].apply(remove_stopwords)\n",
    "\n",
    "# # Tokenize the sentences in the 'Suggestion' column\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].str.lower()\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(word_tokenize)\n",
    "\n",
    "# # Create bigrams or multi-word phrases\n",
    "# bigram = Phrases(df_sug['Suggestion'], min_count=2, threshold=20)\n",
    "# bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# # Apply the bigram phraser to the tokenized text\n",
    "# df_sug['Suggestion'] = df_sug['Suggestion'].apply(lambda tokens: bigram_phraser[tokens])\n",
    "\n",
    "# # Create a dictionary and a corpus for LDA\n",
    "# dictionary = corpora.Dictionary(df_sug['Suggestion'])\n",
    "# corpus = [dictionary.doc2bow(text) for text in df_sug['Suggestion']]\n",
    "\n",
    "# # Train an LDA model\n",
    "# num_topics = 2  # Set the number of topics you want\n",
    "# lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# # Print the top phrases for each topic\n",
    "# for topic_num in range(num_topics):\n",
    "#     topic_words = lda_model.show_topic(topic_num)\n",
    "#     print(f\"Topic {topic_num + 1}:\")\n",
    "#     for word, score in topic_words:\n",
    "#         print(f\"{word}: {score}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "# # df_sug['Suggestion']= df_sug['Suggestion'].apply(remove_stopwords)\n",
    "# # Define a function to extract noun phrases\n",
    "# def extract_noun_phrases(text):\n",
    "#     words = word_tokenize(text)\n",
    "#     tagged = pos_tag(words)\n",
    "#     grammar = r\"\"\"\n",
    "#         NP: {<DT>?<JJ>*<NN.*>+}\n",
    "#     \"\"\"\n",
    "#     cp = RegexpParser(grammar)\n",
    "#     tree = cp.parse(tagged)\n",
    "#     noun_phrases = []\n",
    "#     for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "#         noun_phrase = \" \".join(word for word, tag in subtree.leaves())\n",
    "#         noun_phrases.append(noun_phrase)\n",
    "#     return noun_phrases\n",
    "\n",
    "# # Apply the function to each suggestion\n",
    "# df_sug['Noun_Phrases'] = df_sug['Suggestion'].apply(extract_noun_phrases)\n",
    "\n",
    "# # Print the extracted noun phrases\n",
    "# print(df_sug['Noun_Phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# # Load a pre-trained spaCy model (English)\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# # Extract unique verbs from all suggestions\n",
    "# unique_verbs = set()  # Initialize an empty set to store unique verbs\n",
    "\n",
    "# for suggestion in df_sug['Suggestion']:\n",
    "#     # Process the text with spaCy\n",
    "#     doc = nlp(suggestion)\n",
    "    \n",
    "#     # Extract verbs (tokens with the POS tag \"VERB\") and add them to the set\n",
    "#     extracted_verbs = {token.text for token in doc if token.pos_ == \"VERB\"}\n",
    "    \n",
    "#     # Update the set of unique verbs with the extracted verbs\n",
    "#     unique_verbs.update(extracted_verbs)\n",
    "\n",
    "# # Convert the set of unique verbs back to a list (if needed)\n",
    "# unique_verbs_list = list(unique_verbs)\n",
    "\n",
    "# # Print the unique set of verbs\n",
    "# print(unique_verbs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "\n",
    "# # Load a pre-trained spaCy model (English)\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# # Define a function to filter verbs\n",
    "# def filter_verbs(text):\n",
    "#     doc = nlp(text)\n",
    "#     verb_tokens = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "#     return \" \".join(verb_tokens)\n",
    "\n",
    "# # Apply the filter to the DataFrame\n",
    "# df_sug['Filtered_Text'] = df_sug['Suggestion'].apply(filter_verbs)\n",
    "\n",
    "# # Create a TF-IDF vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=10)  # Adjust the number of features/topics as needed\n",
    "\n",
    "# # Fit and transform the filtered text data\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(df_sug['Filtered_Text'])\n",
    "\n",
    "# # Apply NMF for topic modeling\n",
    "# num_topics = 2  # Set the number of topics you want\n",
    "# nmf_model = NMF(n_components=num_topics)\n",
    "# nmf_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# # Get the top words for each topic\n",
    "# feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# top_words_per_topic = []\n",
    "# for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "#     top_words_idx = topic.argsort()[-5:][::-1]  # Adjust the number of top words per topic as needed\n",
    "#     top_words = [feature_names[i] for i in top_words_idx]\n",
    "#     top_words_per_topic.append(top_words)\n",
    "\n",
    "# # Print the top words for each topic\n",
    "# for topic_idx, top_words in enumerate(top_words_per_topic):\n",
    "#     print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "# df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# # Combine rows into one paragraph\n",
    "# paragraph = ' '.join(df_sug['Suggestion'])\n",
    "\n",
    "# # Load a pre-trained spaCy model (English)\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "# nlp.max_length = 2000000 # or even higher\n",
    "\n",
    "# # Split the paragraph into sentences\n",
    "# doc = nlp(paragraph)\n",
    "# sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# # Initialize a matrix to store sentence similarity scores\n",
    "# num_sentences = len(sentences)\n",
    "# similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "# # Calculate sentence similarity using word vector similarity\n",
    "# for i in range(num_sentences):\n",
    "#     for j in range(i, num_sentences):  # Only calculate each pair once\n",
    "#         if i != j:\n",
    "#             similarity = nlp(sentences[i]).similarity(nlp(sentences[j]))\n",
    "#             similarity_matrix[i, j] = similarity\n",
    "#             similarity_matrix[j, i] = similarity  # Symmetric\n",
    "\n",
    "# # Print the sentence similarity matrix (optional)\n",
    "# print(\"Sentence Similarity Matrix:\")\n",
    "# print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sug= df_disorder.merge(df, on='Concern', how='left')[['Concern','User Concern','Similarity Score','Concern_Sentiment','User_Concern_Sentiment','Suggestion']]\n",
    "df_sug['Suggestion']= df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# Combine rows into one paragraph\n",
    "paragraph = ' '.join(df_sug['Suggestion'])\n",
    "\n",
    "# Load a pre-trained spaCy model (English)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 2000000 # or even higher\n",
    "\n",
    "# Split the paragraph into sentences\n",
    "doc = nlp(paragraph)\n",
    "sentences = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Similarity Matrix:\n",
      "[[0.         0.67193472 0.73401946 ... 0.54987144 0.73435658 0.62735534]\n",
      " [0.67193472 0.         0.80114764 ... 0.81205547 0.70404398 0.79551423]\n",
      " [0.73401946 0.80114764 0.         ... 0.84287786 0.82239437 0.83103955]\n",
      " ...\n",
      " [0.54987144 0.81205547 0.84287786 ... 0.         0.76752687 0.8950268 ]\n",
      " [0.73435658 0.70404398 0.82239437 ... 0.76752687 0.         0.79425073]\n",
      " [0.62735534 0.79551423 0.83103955 ... 0.8950268  0.79425073 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_sug = df_disorder.merge(df, on='Concern', how='left')[['Concern', 'User Concern', 'Similarity Score', 'Concern_Sentiment', 'User_Concern_Sentiment', 'Suggestion']]\n",
    "df_sug['Suggestion'] = df_sug['Suggestion'].astype(str)\n",
    "\n",
    "# Combine rows into one paragraph\n",
    "paragraph = ' '.join(df_sug['Suggestion'])\n",
    "\n",
    "# Load a pre-trained spaCy model (English)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = 2000000  # or even higher\n",
    "\n",
    "# Split the paragraph into sentences\n",
    "doc = nlp(paragraph)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Initialize a matrix to store sentence similarity scores\n",
    "num_sentences = len(sentences)\n",
    "similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "# Calculate sentence similarity using cosine similarity\n",
    "for i in range(num_sentences):\n",
    "    for j in range(i, num_sentences):  # Only calculate each pair once\n",
    "        if i != j:\n",
    "            vector1 = nlp(sentences[i]).vector\n",
    "            vector2 = nlp(sentences[j]).vector\n",
    "            similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity  # Symmetric\n",
    "\n",
    "# Print the sentence similarity matrix (optional)\n",
    "print(\"Sentence Similarity Matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat I think mostly you need to keep talking, no matter how silly it sounds in your head.\n",
      "Top 2: Never in the right place, always feeling like you are demanding to people and feeling like you annoy them.\n",
      "Top 3: You may try, I do this and it helps me a bit, to take a list of what is done to you and what you do to others.\n",
      "Top 4: We don't always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it's worth trying to find out what it is that people don't stay for, and are they really the people you would want to stay for long.\n",
      "Top 5: There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.\n"
     ]
    }
   ],
   "source": [
    "# Sum up the similarity values for each sentence\n",
    "sentence_similarity_scores = np.sum(similarity_matrix, axis=1)\n",
    "\n",
    "# Create a list of (sentence, score) tuples\n",
    "sentence_scores_tuples = [(sentence, score) for sentence, score in zip(sentences, sentence_similarity_scores)]\n",
    "\n",
    "# Sort the tuples by score in descending order\n",
    "sentence_scores_tuples_sorted = sorted(sentence_scores_tuples, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 10 sentences with the highest scores\n",
    "top_10_sentences = [sentence for sentence, score in sentence_scores_tuples_sorted[:5]]\n",
    "\n",
    "# Print the top 10 sentences\n",
    "for i, sentence in enumerate(top_10_sentences, 1):\n",
    "    print(f\"Top {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(top_10_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat We don't always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it's worth trying to find out what it is that people don't stay for, and are they really the people you would want to stay for long. There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Your long text goes here\n",
    "text = ' '.join(top_10_sentences)\n",
    "\n",
    "# Tokenize the text and remove stop words\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc if token.text not in STOP_WORDS]\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "# Calculate sentence scores based on word frequency\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "sentence_scores = {sent: sum([word_freq[word] for word in sent.split()]) for sent in sentences}\n",
    "\n",
    "# Get the top N sentences for summary\n",
    "summary_sentences = [sentence for sentence, score in sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:3]]\n",
    "\n",
    "# Join the summary sentences to create the summary\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat I think mostly you need to keep talking, no matter how silly it sounds in your head.',\n",
       " 'Never in the right place, always feeling like you are demanding to people and feeling like you annoy them.',\n",
       " 'You may try, I do this and it helps me a bit, to take a list of what is done to you and what you do to others.',\n",
       " 'We don\\'t always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it\\'s worth trying to find out what it is that people don\\'t stay for, and are they really the people you would want to stay for long.',\n",
       " 'There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 53.1kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.35MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 6.04MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.67MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:11<00:00, 39.2MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not an Action Phrase: You need to understand yourself  and heal  and accept any past problems, thiscan of course  be a  problem and you need that support that seems to be lacking  at this time BOB Hi, can I just say I feel for you, my husband also suffers severe depression & I know  from experience what your saying.\n",
      "Not an Action Phrase: so I am limited Also' for me' People You have nothing to be sorry for, my dear, you must not take the blame for that which you did not do, though those who are to blame will readily let you burden yourself with their guilt.\n",
      "Not an Action Phrase: There is nothing as difficult as forgiving someone who keeps doing the same thing over and over when they know it is wrong, so I forgave and distanced myself, that way I gave myself time to heal bcz they were out of sight.\n",
      "Not an Action Phrase: You may try, I do this and it helps me a bit, to take a list of what is done to you and what you do to others.\n",
      "Not an Action Phrase: No one will ever understand what going through mental health issues are like, so why bother to dig the hole deeper for yourself.\n",
      "Not an Action Phrase: hi there I think your fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how it‚Äôs effecting your mental health we always fear what we can‚Äôt control don‚Äôt we it will pass with the right help  You can always ring the Samaritans they pretty good at talking to us when we anxious  Take care  Nat I think mostly you need to keep talking, no matter how silly it sounds in your head.\n",
      "Not an Action Phrase: Never in the right place, always feeling like you are demanding to people and feeling like you annoy them.\n",
      "Not an Action Phrase: Stepping back to take stock of all you are dealing with, in a way that acknowledges your incredible strength is important because it shows you that you are not as fragile as you may feel.\n",
      "Not an Action Phrase: We don't always know how we seem to other people;  someone once said that I was \"aloof\" - \"stand offish\", when actually I was shy, so it's worth trying to find out what it is that people don't stay for, and are they really the people you would want to stay for long.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph= ' '.join(top_10_sentences)\n",
    "# Split paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Define a threshold for action phrase detection\n",
    "threshold = 0.5\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    # Tokenize and convert to tensor\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True, max_length=64, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        action_probability = torch.softmax(outputs.logits, dim=1)[0][1].item()\n",
    "\n",
    "    # Check if the sentence contains an action phrase\n",
    "    is_action_phrase = action_probability > threshold\n",
    "\n",
    "    if is_action_phrase:\n",
    "        print(\"Action Phrase Detected:\", sentence)\n",
    "    else:\n",
    "        print(\"Not an Action Phrase:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>Concern</th>\n",
       "      <th>Suggestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Depression/OCD crisis</td>\n",
       "      <td>obsessive-compulsive-disorder-ocd</td>\n",
       "      <td>trigger warning‚ä¶,having a breakdown of crisi...</td>\n",
       "      <td>I feel that. I was for real having a breakdown...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                tag  \\\n",
       "2  Depression/OCD crisis  obsessive-compulsive-disorder-ocd   \n",
       "\n",
       "                                             Concern  \\\n",
       "2  trigger warning‚ä¶,having a breakdown of crisi...   \n",
       "\n",
       "                                          Suggestion  \n",
       "2  I feel that. I was for real having a breakdown...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Phrases: ['need to understand', 'heal', 'accept', 'be', 'need', 'seems to be lacking', 'can', 'just say', 'feel', 'also suffers', 'know', 'so', 'am', 'Also', 'have', 'to be', 'must not take', 'did not do', 'are to blame will readily let', 'burden', 'is', 'as', 'forgiving', 'keeps doing', 'over', 'know', 'is', 'so', 'forgave', 'distanced', 'gave', 'to heal', 'were', 'may try', 'do', 'helps', 'to take', 'is done', 'do', 'will ever understand', 'going', 'are', 'so', 'bother to dig', 'deeper', 'there', 'think', 'fearing', 'going', 'right now', 'are', 'get', 'tell', 'it‚Äôs effecting', 'always fear', 'can‚Äôt', 'will pass', 'can always ring', 'pretty', 'talking', 'Take', 'think mostly', 'need to keep talking', 'no matter', 'sounds', 'Never', 'always feeling', 'are demanding', 'feeling', 'annoy', 'Stepping back to take', 'are dealing', 'acknowledges', 'is', 'shows', 'are not as', 'may feel', \"do n't always know\", 'seem', 'once said', 'was', 'actually', 'was', \"'s\", 'trying to find', 'is', \"do n't stay\", 'are', 'really', 'would want to stay', 'long']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Sample text\n",
    "text = ' '.join(top_10_sentences)\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define a list of action-related entity labels\n",
    "action_labels = [\"VERB\", \"AUX\", \"ADV\", \"PART\"]\n",
    "\n",
    "# Initialize a list to store action phrases\n",
    "action_phrases = []\n",
    "\n",
    "# Initialize variables to track phrases\n",
    "current_phrase = \"\"\n",
    "phrase_tokens = []\n",
    "\n",
    "# Iterate through tokens and extract action-related phrases\n",
    "for token in doc:\n",
    "    if token.pos_ in action_labels:\n",
    "        # If the token is an action-related word, add it to the current phrase\n",
    "        phrase_tokens.append(token.text)\n",
    "    else:\n",
    "        # If the token is not an action-related word, check if the current phrase is non-empty\n",
    "        if phrase_tokens:\n",
    "            # Join the tokens to form a phrase and add it to the list of action phrases\n",
    "            action_phrases.append(\" \".join(phrase_tokens))\n",
    "            # Reset variables for the next phrase\n",
    "            phrase_tokens = []\n",
    "\n",
    "# Check if there's a remaining phrase at the end\n",
    "if phrase_tokens:\n",
    "    action_phrases.append(\" \".join(phrase_tokens))\n",
    "\n",
    "# Print extracted action phrases\n",
    "print(\"Action Phrases:\", action_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 2.91MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 6.18MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 13.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 5.37MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 9.52MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 308M/308M [00:07<00:00, 43.9MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 1.30MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think u are fearing the future because with everything going on right now in the world many of us are at the minute but please get to your gp and tell them how itôs effecting your mental health we always fear what we canôt control donrou we it will pass with the right help\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the FLAN-T5-Small model and tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Define your input sentences\n",
    "input_sentences = top_10_sentences\n",
    "\n",
    "# Combine the input sentences into one string\n",
    "input_text = \" \".join(input_sentences)\n",
    "\n",
    "# Tokenize and generate the message\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "output_ids = model.generate(input_ids, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the generated message\n",
    "generated_message = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
